{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from transformers import MarkupLMFeatureExtractor, MarkupLMProcessor, MarkupLMForTokenClassification\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_path = \"title_date_tag.pth\"\n",
    "segmentation_model_path = \"segmentation_model.pth\"\n",
    "\n",
    "allowed_labels = [\"title\", \"short_text\", \"date\", \"time\", \"tag\", \"short_title\", \"author\"]\n",
    "\n",
    "\n",
    "class_label2id = {\"OTHER\" : 0,\n",
    "            \"title\" : 1, \n",
    "            \"short_text\" : 0, \n",
    "            \"date\" : 2, \n",
    "            \"time\" : 2, \n",
    "            \"tag\" : 3, \n",
    "            \"short_title\" : 0, \n",
    "            \"author\" : 0}\n",
    "\n",
    "class_id2label = {0: \"OTHER\",\n",
    "            1 : \"title\",\n",
    "            2 : \"date\",\n",
    "            3 : \"tag\"}\n",
    "\n",
    "colors = {1 : \"blue\",\n",
    "          2 : \"purple\",\n",
    "          3 : \"brown\"}\n",
    "\n",
    "\n",
    "block_label2id = {\"BEGIN\": 1, \"OTHER\": 0}\n",
    "\n",
    "block_id2label = {1: \"BEGIN\", 0: \"OTHER\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициалиация датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Model Loaded\n",
      "Segmentation Model Loaded\n"
     ]
    }
   ],
   "source": [
    "classification_model = MarkupLMForTokenClassification.from_pretrained(\"microsoft/markuplm-base\", id2label=class_id2label, label2id=class_label2id)\n",
    "segmentation_model = MarkupLMForTokenClassification.from_pretrained(\"microsoft/markuplm-base\", id2label=block_id2label, label2id=block_label2id)\n",
    "\n",
    "if os.path.exists(classification_model_path):\n",
    "    classification_model.load_state_dict(torch.load(classification_model_path))\n",
    "    print(\"Classification Model Loaded\")\n",
    "else:\n",
    "    raise Exception(\"No model found\")\n",
    "\n",
    "\n",
    "if os.path.exists(segmentation_model_path):\n",
    "    segmentation_model.load_state_dict(torch.load(segmentation_model_path))\n",
    "    print(\"Segmentation Model Loaded\")\n",
    "else:\n",
    "    raise Exception(\"No model found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import lxml\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from metrics import generate_segmentation_str, path_contains\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "from urllib.request import Request, urlopen\n",
    "from lxml.html.clean import Cleaner\n",
    "import os\n",
    "import webbrowser\n",
    "\n",
    "optimizer = AdamW(classification_model.parameters(), lr=2e-5)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "classification_model.to(device)\n",
    "segmentation_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "from argostranslate.translate import get_installed_languages\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import random\n",
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import lxml\n",
    "import os\n",
    "import sys\n",
    "import time \n",
    "\n",
    "from lxml import etree\n",
    "from lxml.html.clean import Cleaner\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(text):\n",
    "    return \" \".join(re.split(r\"\\s+\", text.strip()))\n",
    "\n",
    "\n",
    "def clean_format_str(text):\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
    "    text = clean_spaces(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_dom_tree(html, need_clean):\n",
    "    if need_clean:\n",
    "        cleaner = Cleaner()\n",
    "        cleaner.scripts = True\n",
    "        cleaner.javascript = True\n",
    "        cleaner.comments = True\n",
    "        cleaner.style = True\n",
    "        cleaner.inline_style = False\n",
    "        cleaner.links = False\n",
    "        cleaner.meta = False\n",
    "        cleaner.page_structure = False\n",
    "        cleaner.processing_instructions = True\n",
    "        cleaner.embedded = False\n",
    "        cleaner.frames = False\n",
    "        cleaner.forms = False\n",
    "        cleaner.annoying_tags = True\n",
    "        cleaner.remove_unknown_tags = False\n",
    "        cleaner.safe_attrs_only = False\n",
    "        cleaner.add_nofollow = False\n",
    "        \n",
    "        html = html.replace(\"\\0\", \"\")  # Delete NULL bytes\n",
    "        html = clean_format_str(html)\n",
    "        x = lxml.html.fromstring(html)\n",
    "        etree_root = cleaner.clean_html(x)\n",
    "        dom_tree = etree.ElementTree(etree_root)\n",
    "    else:\n",
    "        dom_tree = lxml.html.fromstring(html).getroottree()\n",
    "    return dom_tree\n",
    "\n",
    "def ru2en(text, translator):\n",
    "\n",
    "    translated_text = translator.translate(text)\n",
    "    return translated_text\n",
    "\n",
    "def translate_html(html_str, translator, need_clean=False, from_code='auto', to_code=\"en\"):\n",
    "    tree = get_dom_tree(html_str, need_clean)\n",
    "    tasks = []\n",
    "    for e in tree.iter():\n",
    "        if e.text:\n",
    "            node = unicodedata.normalize('NFKD', e.text)\n",
    "            e.text = ru2en(node, translator)\n",
    "        if e.tail:\n",
    "            node = unicodedata.normalize('NFKD', e.tail)\n",
    "            e.tail = ru2en(node, translator)\n",
    "            \n",
    "    return lxml.html.tostring(tree, doctype=\"<!DOCTYPE html>\", encoding='unicode')\n",
    "\n",
    "from_code = \"ru\"\n",
    "to_code = \"en\"\n",
    "argostranslate.package.update_package_index()\n",
    "available_packages = argostranslate.package.get_available_packages()\n",
    "package_to_install = next(\n",
    "    filter(\n",
    "        lambda x: x.from_code == from_code and x.to_code == to_code, available_packages\n",
    "    )\n",
    ")\n",
    "argostranslate.package.install_from_path(package_to_install.download())\n",
    "\n",
    "ru, en = get_installed_languages()\n",
    "translator = en.get_translation(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://life.ru/s/novosti\n",
      "['/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[1]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[1]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[2]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[2]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[2]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[3]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[3]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[3]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[3]/div[2]/div[1]/ul/li[4]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[3]/div[2]/div[1]/ul/li[5]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[4]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[4]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[4]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[4]/div[2]/div[1]/ul/li[4]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[4]/div[2]/div[1]/ul/li[5]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[5]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[6]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[6]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[6]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[7]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[7]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[8]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[1]/div/div[4]/div/div/a[8]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[1]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[1]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[1]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[2]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[2]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[3]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[3]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[4]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[4]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[5]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[5]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[6]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[6]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[6]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[7]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[7]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[8]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[8]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[9]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[9]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[11]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[11]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[11]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[12]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[12]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[13]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[13]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[13]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[14]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[14]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[15]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[15]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[15]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[15]/div[2]/div[1]/ul/li[4]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[16]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[16]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[16]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[16]/div[2]/div[1]/ul/li[4]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[17]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[17]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[17]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[18]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[18]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[19]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[19]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[19]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[20]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[20]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[21]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[21]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[21]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[21]/div[2]/div[1]/ul/li[4]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[21]/div[2]/div[1]/ul/li[5]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[22]/div[2]/div[1]/ul/li[1]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[22]/div[2]/div[1]/ul/li[2]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[22]/div[2]/div[1]/ul/li[3]', '/html/body/div[2]/div/div[3]/div[2]/div[2]/div/div[3]/div[1]/div/div/div/a[22]/div[2]/div[1]/ul/li[4]']\n",
      "Segmentation Done\n",
      "Classification Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown url type: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[0;32m---> 18\u001b[0m req \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-Agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMozilla/5.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m html_response \u001b[38;5;241m=\u001b[39m urlopen(req)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m     24\u001b[0m tree \u001b[38;5;241m=\u001b[39m lxml\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39mfromstring(html_response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/urllib/request.py:320\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[0;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    318\u001b[0m              origin_req_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unverifiable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    319\u001b[0m              method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_url\u001b[49m \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munredirected_hdrs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/urllib/request.py:346\u001b[0m, in \u001b[0;36mRequest.full_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url \u001b[38;5;241m=\u001b[39m unwrap(url)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfragment \u001b[38;5;241m=\u001b[39m _splittag(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/urllib/request.py:375\u001b[0m, in \u001b[0;36mRequest._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype, rest \u001b[38;5;241m=\u001b[39m _splittype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown url type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_url)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselector \u001b[38;5;241m=\u001b[39m _splithost(rest)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost:\n",
      "\u001b[0;31mValueError\u001b[0m: unknown url type: ''"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "classification_model.eval()\n",
    "segmentation_model.eval()\n",
    "\n",
    "extractor = MarkupLMFeatureExtractor()\n",
    "valid_processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
    "valid_processor.parse_html = False\n",
    "\n",
    "\n",
    "experiment_results = defaultdict(int)\n",
    "\n",
    "all_true_results = 0\n",
    "all_extracted_results = 0\n",
    "ious = []\n",
    "while(True):\n",
    "    \"Insert your url:\"\n",
    "    url = input()\n",
    "    print(url)\n",
    "    req = Request(\n",
    "        url=url, \n",
    "        headers={'User-Agent': 'Mozilla/5.0'}\n",
    "    )\n",
    "    html_response = urlopen(req).read().decode()\n",
    "    \n",
    "    tree = lxml.html.fromstring(html_response)\n",
    "    html_response = translate_html(html_response, translator)\n",
    "    item = extractor(html_response)\n",
    "    nodes, xpaths = item['nodes'], item['xpaths']\n",
    "    \n",
    "\n",
    "    block_encoding = valid_processor(nodes=nodes, xpaths=xpaths, stride=0,\n",
    "                               padding=\"max_length\", truncation=True, return_tensors=\"pt\", \n",
    "                               return_overflowing_tokens=True, return_offsets_mapping=True)\n",
    "    \n",
    "    inputs = {k:v.to(device) for k,v in block_encoding.items()}\n",
    "    \n",
    "    inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        segmentation_output = segmentation_model(**inputs)\n",
    "    segmentation_predictions = segmentation_output.logits.argmax(dim=-1)\n",
    "    \n",
    "    \n",
    "    pred_block_xpaths = []\n",
    "    \n",
    "    pred_block_xpaths = []\n",
    "    \n",
    "    statistic = {k: defaultdict(int) for k in block_id2label.keys()}\n",
    "    \n",
    "    for idx in range(len(segmentation_predictions)):\n",
    "        for pred_id, word_id, offset in zip(segmentation_predictions[idx].tolist(), block_encoding.word_ids(idx), offset_mapping[idx].tolist()):\n",
    "            if word_id is not None and offset[0] == 0:\n",
    "                if pred_id == 1:\n",
    "                    suffix = xpaths[0][word_id]\n",
    "                    suffix = re.sub(r\"\\[\\d*\\]\", \"\", suffix)\n",
    "                    statistic[pred_id][suffix] += 1\n",
    "                    \n",
    "    allowed_suffix = dict()\n",
    "    \n",
    "    for label in statistic.keys():\n",
    "        suffix = max(statistic[label], key=statistic[label].get, default=\"\")\n",
    "        allowed_suffix[suffix] = label\n",
    "              \n",
    "    for idx in range(len(segmentation_predictions)):\n",
    "        for pred_id, word_id, offset in zip(segmentation_predictions[idx].tolist(), block_encoding.word_ids(idx), offset_mapping[idx].tolist()):\n",
    "            if word_id is not None and offset[0] == 0:\n",
    "                suffix = xpaths[0][word_id]\n",
    "                suffix = re.sub(r\"\\[\\d*\\]\", \"\", suffix)             \n",
    "                \n",
    "                if (suffix in allowed_suffix):\n",
    "                    pred_block_xpaths += [xpaths[0][word_id]]\n",
    "\n",
    "    pred_block_prefix = generate_segmentation_str(pred_block_xpaths)\n",
    "    for xpath in pred_block_prefix:\n",
    "        # print(xpath)\n",
    "        try:\n",
    "            element = tree.xpath(xpath)[0]\n",
    "            element.set(\"style\", \"border:dashed; border-color: green\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(pred_block_prefix)\n",
    "    print(\"Segmentation Done\")\n",
    "    \n",
    "    # ^^^^^ Main result of segmentation\n",
    "    \n",
    "    #CLASSIFICATION\n",
    "    nodes, xpaths = item['nodes'], item['xpaths']\n",
    "    \n",
    "\n",
    "    class_encoding = block_encoding\n",
    "    \n",
    "    inputs = {k:v.to(device) for k,v in class_encoding.items()}\n",
    "    \n",
    "    inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        classification_output = classification_model(**inputs)\n",
    "    classification_predictions = classification_output.logits.argmax(dim=-1)\n",
    "    print(\"Classification Done\")\n",
    "    \n",
    "    predicted_entities = defaultdict(list)\n",
    "\n",
    "    out_xpaths = defaultdict(dict)\n",
    "    \n",
    "    statistic = {k: defaultdict(int) for k in class_id2label.keys()}\n",
    "    \n",
    "    for idx in range(len(classification_predictions)):\n",
    "        for pred_id, word_id, offset in zip(classification_predictions[idx].tolist(), class_encoding.word_ids(idx), offset_mapping[idx].tolist()):\n",
    "            if word_id is not None and offset[0] == 0:\n",
    "                \n",
    "                in_predicted_blocks = [path_contains(block_xpath.split('/'), xpaths[0][word_id].split('/')) for block_xpath in pred_block_prefix]\n",
    "\n",
    "                if pred_id != 0 and any(in_predicted_blocks):\n",
    "                    \n",
    "                    suffix = xpaths[0][word_id]\n",
    "                    suffix = re.sub(r\"\\[\\d*\\]\", \"\", suffix)\n",
    "                    statistic[pred_id][suffix] += 1\n",
    "                    \n",
    "    allowed_suffix = dict()\n",
    "    \n",
    "    for label in statistic.keys():\n",
    "        suffix = max(statistic[label], key=statistic[label].get, default=\"\")\n",
    "        allowed_suffix[suffix] = label\n",
    "    \n",
    "    \n",
    "    for idx in range(len(classification_predictions)):\n",
    "        for pred_id, word_id, offset, probability in zip(classification_predictions[idx].tolist(), class_encoding.word_ids(idx), offset_mapping[idx].tolist(), classification_output.logits[idx]):\n",
    "            if word_id is not None and offset[0] == 0:\n",
    "                \n",
    "                in_predicted_blocks = [path_contains(block_xpath.split('/'), xpaths[0][word_id].split('/')) for block_xpath in pred_block_prefix]\n",
    "\n",
    "                suffix = re.sub(r\"\\[\\d*\\]\", \"\", xpaths[0][word_id])\n",
    "\n",
    "                if (suffix in allowed_suffix) and any(in_predicted_blocks):\n",
    "                    try:\n",
    "                        element = tree.xpath(xpaths[0][word_id])[0]\n",
    "                        element.set(\"style\", f\"border:dashed; border-color: {colors[pred_id]}\")\n",
    "                        element.set(\"probs\", f\"{probability.tolist()}\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    \n",
    "    # for idx in range(len(classification_predictions)):\n",
    "    #     for pred_id, word_id, offset in zip(classification_predictions[idx].tolist(), class_encoding.word_ids(idx), offset_mapping[idx].tolist()):\n",
    "    #         if word_id is not None and offset[0] == 0:\n",
    "                \n",
    "    #             in_predicted_blocks = [path_contains(block_xpath.split('/'), xpaths[0][word_id].split('/')) for block_xpath in pred_block_prefix]\n",
    "\n",
    "    #             if pred_id != 0 and any(in_predicted_blocks):\n",
    "    #                 try:\n",
    "    #                     element = tree.xpath(xpaths[0][word_id])[0]\n",
    "    #                     element.set(\"style\", f\"border:dashed; border-color: {colors[pred_id]}\")\n",
    "                        \n",
    "    #                     statistic[pred_id].append(xpaths[0][word_id].split('/')[-4:])\n",
    "    #                 except Exception:\n",
    "    #                     pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    path = os.path.abspath('temp.html')\n",
    "    url = 'file://' + path\n",
    "\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        # print(etree.tostring(tree, pretty_print=True).decode(\"utf-8\"))\n",
    "        f.write(lxml.html.tostring(tree, pretty_print=True, encoding='unicode', doctype=\"<!DOCTYPE html>\"))\n",
    "        # f.write(html)\n",
    "    webbrowser.open(url)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "markup-segmentation-03k7_eFX-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
