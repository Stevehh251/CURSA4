{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from transformers import MarkupLMFeatureExtractor, MarkupLMProcessor, MarkupLMForTokenClassification\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "fout_model = \"ext_segmentation.pth\"\n",
    "\n",
    "allowed_labels = [\"title\", \"short_text\", \"date\", \"time\", \"tag\", \"short_title\", \"author\"]\n",
    "\n",
    "\n",
    "id2label = {1: \"BEGIN\", 0: \"OTHER\"}\n",
    "label2id = {\"BEGIN\": 1, \"OTHER\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_folder(folder_path : str):\n",
    "    '''\n",
    "        This function loading all json files from folder.\n",
    "        Each file contains dict with labels and its values.\n",
    "        Each file must contains \"html\" label with its html code. \n",
    "        Each file must contains \"xpaths\" label with its labeled xpaths list. \n",
    "        \n",
    "    '''\n",
    "    extractor = MarkupLMFeatureExtractor()\n",
    "    \n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    files_path = glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file_path in tqdm(files_path):\n",
    "        # print(file_path)\n",
    "        with open(file_path) as file:\n",
    "            info = json.load(file)\n",
    "            \n",
    "        html = info[\"html\"]\n",
    "        labeled_xpaths = info[\"xpaths\"]\n",
    "\n",
    "        encoding = extractor(html)\n",
    "            \n",
    "        \n",
    "        labels = []\n",
    "        \n",
    "        for xpath in encoding[\"xpaths\"][0]:\n",
    "            if xpath in labeled_xpaths:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "\n",
    "        finded_segments = [_ for _ in labels if _ != 0]\n",
    "        if len(finded_segments) == 0:\n",
    "            print(file_path)\n",
    "            with open(\"labeled_xpaths\", \"w\") as f:\n",
    "                print(*labeled_xpaths, sep='\\n', file=f)\n",
    "            with open(\"xpaths\", \"w\") as f:\n",
    "                print(*encoding[\"xpaths\"][0], sep='\\n', file=f)\n",
    "                \n",
    "            raise Exception(\"No blocks found\")\n",
    "        \n",
    "        \n",
    "        # print(len(labels))\n",
    "        # print([_ for _ in labels if _ != 0])\n",
    "        \n",
    "        labels = [labels]\n",
    "        # print(len(encoding['nodes'][0]), len(encoding['xpaths'][0]), len(labels[0]))\n",
    "        data.append({'nodes': encoding['nodes'],\n",
    "                     'xpaths': encoding['xpaths'],\n",
    "                     'node_labels': labels,\n",
    "                     'html': html})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bad_folder(folder_path : str):\n",
    "    '''\n",
    "        This function loading all json files from folder.\n",
    "        Each file contains dict with labels and its values.\n",
    "        Each file must contains \"html\" label with its html code. \n",
    "        Each file must contains \"xpaths\" label with its labeled xpaths list. \n",
    "        \n",
    "    '''\n",
    "    extractor = MarkupLMFeatureExtractor()\n",
    "    \n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    files_path = glob(os.path.join(folder_path, \"*.html\"))\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file_path in tqdm(files_path):\n",
    "        # print(file_path)\n",
    "        with open(file_path) as file:\n",
    "            info = file.read()\n",
    "            \n",
    "        html = info\n",
    "\n",
    "        encoding = extractor(html)\n",
    "            \n",
    "        labels = []\n",
    "        \n",
    "        for xpath in encoding[\"xpaths\"][0]:\n",
    "            labels.append(0)\n",
    "\n",
    "        labels = [labels]\n",
    "        \n",
    "        data.append({'nodes': encoding['nodes'],\n",
    "                     'xpaths': encoding['xpaths'],\n",
    "                     'node_labels': labels,\n",
    "                     'html': html})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_from_folder(\"test_dataset/train_part\")\n",
    "train_data += load_bad_folder(\"test_dataset/bad\")\n",
    "valid_data = load_from_folder(\"test_dataset/test_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size : \", len(train_data))\n",
    "print(\"Test size : \", len(valid_data))\n",
    "print(\"Train proportion : \", len(train_data) / (len(valid_data) + len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 256\n",
    "for node, label in zip(valid_data[idx]['nodes'][0], valid_data[idx]['node_labels'][0]):\n",
    "  if id2label[label] == 'title':\n",
    "    print(node, id2label[label])\n",
    "  # print(node, id2label[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициалиация датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkupLMDataset(Dataset):\n",
    "    \"\"\"Dataset for token classification with MarkupLM.\"\"\"\n",
    "\n",
    "    def __init__(self, data, processor=None):\n",
    "        self.processor = processor\n",
    "        newdata = []\n",
    "        for item in tqdm(data):\n",
    "            nodes, xpaths, node_labels = item['nodes'], item['xpaths'], item['node_labels']\n",
    "            encoding = self.processor(nodes=nodes, xpaths=xpaths, stride=200, node_labels=node_labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True, return_offsets_mapping=True)\n",
    "            \n",
    "            # encoding['block_xpaths'] = np.array([item['block_xpaths']] * len(encoding['labels']))\n",
    "            # encoding['all_xpaths'] = np.array([item['xpaths']] * len(encoding['labels']))\n",
    "            \n",
    "            for idx in range(len(encoding['labels'])):\n",
    "                newdata += [{k: v[idx].squeeze() for k, v in encoding.items()}]\n",
    "\n",
    "        self.data = newdata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # first, get nodes, xpaths and node labels\n",
    "        item = self.data[idx]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\", truncation = True)\n",
    "processor.parse_html = False\n",
    "\n",
    "train_set = MarkupLMDataset(data=train_data, processor=processor)\n",
    "valid_set = MarkupLMDataset(data=valid_data, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = valid_set[9]\n",
    "for k,v in example.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, label in zip(example['input_ids'].tolist(), example['labels'].tolist()):\n",
    "    if label != -100:\n",
    "        print(processor.decode([id]), label)\n",
    "    # if label == 1:\n",
    "    #     print(processor.decode([id]), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MarkupLMForTokenClassification.from_pretrained(\"microsoft/markuplm-base\", id2label=id2label, label2id=label2id)\n",
    "\n",
    "if os.path.exists(fout_model):\n",
    "    model.load_state_dict(torch.load(fout_model))\n",
    "    print(\"Model Loaded\")\n",
    "else:\n",
    "    print(\"Its new model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "best_metric = 0\n",
    "\n",
    "train_history = []\n",
    "test_history = []\n",
    "\n",
    "def train_model(): \n",
    "    model.train()\n",
    "\n",
    "    labels_true = []\n",
    "    labels_predicted = []\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch.pop(\"overflow_to_sample_mapping\")\n",
    "        batch.pop(\"offset_mapping\")\n",
    "        inputs = {k:v.to(device) for k,v in batch.items()}\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"Loss:\", loss.item())\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        labels_predicted += predictions[0].tolist()\n",
    "        labels_true += inputs[\"labels\"][0].tolist()\n",
    "\n",
    "    score = classification_report(labels_true, labels_predicted, output_dict=True, zero_division=0)['1']\n",
    "    with open(\"out_log.txt\", \"a\") as logfile:\n",
    "        print(datetime.datetime.now(), file=logfile)\n",
    "        print(\"Train : \\n\", score, file=logfile)\n",
    "\n",
    "    score = classification_report(labels_true, labels_predicted, output_dict=True, zero_division=0)['1']['f1-score']\n",
    "    train_history.append(score)\n",
    "    with open(\"train_history.json\", \"w\") as f:\n",
    "        json.dump(train_history, f)\n",
    "\n",
    "    print(f\"Train : {score}\")\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    model.eval()\n",
    "\n",
    "    global best_metric\n",
    "    labels_true = []\n",
    "    labels_predicted = []\n",
    "\n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        # get the inputs;\n",
    "        batch.pop(\"overflow_to_sample_mapping\")\n",
    "        batch.pop(\"offset_mapping\")\n",
    "        inputs = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "    \n",
    "        # forward + backward + optimize\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        labels_predicted += predictions[0].tolist()\n",
    "        labels_true += inputs[\"labels\"][0].tolist()\n",
    "\n",
    "    score = classification_report(labels_true, labels_predicted, output_dict=True, zero_division=0)['1']\n",
    "    with open(\"out_log.txt\", \"a\") as logfile:\n",
    "        print(datetime.datetime.now(), file=logfile)\n",
    "        print(\"Test : \\n\", score, file=logfile)\n",
    "\n",
    "    score = classification_report(labels_true, labels_predicted, output_dict=True, zero_division=0)['1']['f1-score']\n",
    "    if score > best_metric:\n",
    "        best_metric = score     \n",
    "        torch.save(model.state_dict(), fout_model)\n",
    "\n",
    "    test_history.append(score)\n",
    "    with open(\"test_history.json\", \"w\") as f:\n",
    "        json.dump(test_history, f)\n",
    "    print(f\"Test : {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\", truncation = True)\n",
    "test_processor.parse_html = False\n",
    "\n",
    "model.to(torch.device(\"cuda\"))\n",
    "model.eval()\n",
    "print(device)\n",
    "\n",
    "valid_metric = segmentation_metric()\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "valid_processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
    "valid_processor.parse_html = False\n",
    "\n",
    "    \n",
    "for record in tqdm(valid_data):\n",
    "\n",
    "    item = record\n",
    "    nodes, xpaths, node_labels = item['nodes'], item['xpaths'], item['node_labels']\n",
    "    \n",
    "    encoding = valid_processor(nodes=nodes, xpaths=xpaths, stride=200, node_labels=node_labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True, return_offsets_mapping=True)\n",
    "    input = {k:v.to(device) for k,v in encoding.items()}\n",
    "    \n",
    "    input.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = input.pop(\"offset_mapping\")\n",
    "    labels = input.pop(\"labels\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input)\n",
    "        \n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "    pred_xpaths = []\n",
    "    true_xpaths = [xpath for idx, xpath in enumerate(xpaths[0]) if node_labels[0][idx] == 1]    \n",
    "    all_xpaths = []\n",
    "    probs = []\n",
    "    \n",
    "    for idx in range(len(predictions)):\n",
    "        for pred_id, word_id, offset, label_id, probability in zip(predictions[idx].tolist(), encoding.word_ids(idx), offset_mapping[idx].tolist(), labels[idx].tolist(), outputs.logits[idx]):\n",
    "            if word_id is not None and offset[0] == 0:\n",
    "                true_labels += [label_id]\n",
    "                predicted_labels += [pred_id]\n",
    "                if (pred_id == 1):\n",
    "                    pred_xpaths += [xpaths[0][word_id]]\n",
    "                all_xpaths += [xpaths[0][word_id]]\n",
    "                probs += [(idx, probability.tolist())]\n",
    "                \n",
    "    # if len(pred_xpaths) == 0:\n",
    "    #     print(list(zip(all_xpaths, probs)))  \n",
    "    #     print(true_xpaths)    \n",
    "    \n",
    "    valid_metric.add_result({\"true_xpaths\" : true_xpaths,\n",
    "                             \"pred_xpaths\" : pred_xpaths,\n",
    "                            #  \"html\": item[\"html\"],\n",
    "                             \"all_xpaths\" : all_xpaths})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation score : \")\n",
    "print(*valid_metric.get_metric().items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results without 'bad' htmls : \")\n",
    "ari = [score[\"ARI\"] for score in valid_metric.ARI_NMI if score[\"ARI\"] > 0.01]\n",
    "nmi = [score[\"NMI\"] for score in valid_metric.ARI_NMI if score[\"ARI\"] > 0.01]\n",
    "print(\"ARI : \", sum(ari) / len(ari))\n",
    "print(\"NMI : \", sum(nmi) / len(nmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ari = [score[\"ARI\"] for score in valid_metric.ARI_NMI if score[\"ARI\"] <= 0.01]\n",
    "print(len(bad_ari))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label-marking scores :\")\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "markup-segmentation-03k7_eFX-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
